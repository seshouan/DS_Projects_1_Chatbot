{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 21:35:43.563953: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import required modules\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a json file\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "intents_file = open('intents.json').read()\n",
    "intents = json.loads(intents_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing: tokenizer\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_letters = list('!?,.')\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word\n",
    "        word = nltk.word_tokenize(pattern)\n",
    "        words.extend(word)\n",
    "        # add documents to the corpus\n",
    "        documents.append((word, intent['tag']))\n",
    "        # add to the classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing: lemmatizer\n",
    "\n",
    "# lemmatize and lowercase each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_letters]\n",
    "words = sorted(list(set(words)))\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# print number of documents, which combine patterns and intents\n",
    "print(len(documents), 'documents')\n",
    "# print classes as intents\n",
    "print(len(classes), 'classes', classes)\n",
    "# print all words in the vocabulary\n",
    "print(len(words), 'unique lemmatized words', words)\n",
    "\n",
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input patterns to numbers\n",
    "\n",
    "# create the training data\n",
    "training = []\n",
    "# create output array\n",
    "output_empty = [0] * len(classes)\n",
    "# use a bag of words for every sentence as the training set\n",
    "for doc in documents:\n",
    "    # init a bag of words\n",
    "    bag = []\n",
    "    # get the list of tokenized words for the pattern\n",
    "    word_pattern = doc[0]\n",
    "    # lemmatize each word\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    # flag words that appear in the current pattern as 1s the rest as 0s\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "\n",
    "    # output 1s for the current tag and 0s otherwise\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "# shuffle the training set and turn it to a numpy array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "# create training inputs (patterns) and label (intent) sets\n",
    "train_X = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "print('created training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model with 3 layers deep, using dropout layers and the SGD optimizer; train 200 epochs\n",
    "\n",
    "# define the deep neural net\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_X[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "# compile the model using SGD with Nesterov accelerated gradient\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# train and save the model\n",
    "hist = model.fit(np.array(train_X), np.array(train.y), epochs=200, batch_size=5, verbose=1)\n",
    "model.save('chatbot_model.h5', hist)\n",
    "\n",
    "print('created model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
